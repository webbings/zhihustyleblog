---
layout: post
title: MachineLearning笔记-线性模型
author: Jason
date:   2017-03-04
categories: ComputerVision
permalink: /archivers/ML_LinearModel
---



<p class="lead">学习周志华老师的《机器学习》笔记之线性模型Linear Model</p>

线性模型说简单简单，说复杂复杂，也要区分使用场景和解释方式。不论怎么解释，线性模型是机器学习的基础。
> 线性模型形式简单、易于建模，但却蕴含着机器学习中的一些重要的基本思想，许多功能更为强大的非线性模型（nonlinear model）可在线性模型的基础上通过引入层级结构或高维映射而得。

还记得吴恩达教授引用的例子。即房价和房子面积的例子。我们可以粗略的认为房价和面积的关系是呈现 <b>y=kx+b</b> 的线性关系（实际上随着面积的增加，房价会趋于平缓，可能是一个曲线）。这就是典型的线性模型。
如果有多个属性，那么就可以描述成 <b>y=k<sub>1</sub>x<sub>1</sub>+k<sub>2</sub>x<sub>2</sub>+...k<sub>n-1</sub>x<sub>n-1</sub>+k<sub>n</sub>x<sub>n</sub>+b</b> 。

举例：之前推荐系统的原理，实际本质很简单，涉及2个属性：1是相似度sim，2是信任值trust。可以理解为房价的2个影响因素：面积+地段。
简单的描述，我的工作就是根据输入面积+地段计算输出房价。
我之前设计了多种模型，其中有一种类似线性模型如下：y=(sim+k1*trust)/k2 (1<k1<k2)，其实就是一种线性模型。由于场景不同，我只需要计算一个联系的强弱属于[0,1]。

书中举例了一些不同的形式，并且用数学形式表达，难度不小。
介绍如下：

* 线性回归
定义（what）：
> “线性回归”（linear regression）试图学得一个线性模型以尽可能准确地预测实值输出标记。

> <b>f(x<sub>i</sub>) = wx<sub>i</sub> + b</b>

然后根据f(x<sub>i</sub>)和真实值y<sub>i</sub>之间的比较，来计算w和b。主要的方法有均方误差等，然后我们可以扩大到多维（多个属性：例如面积、地段、楼层......），就可以用矩阵来描述了。
一个简写的线性回归模型为：

> <b>y = w<sup>T</sup>x + b</b>

看上去简单，却有着丰富的变化。
广义线性模型描述如下：
> <b>y = g<sup>-1</sup>(w<sup>T</sup>x + b)</b>

其中函数g(·)考虑为单调可微函数。

* 对数几率回归

线性回归作用是进行学习预测结果，对数几率回归作用是分类任务。如分为2类：0和1；或者扩展为多分类。名字虽然是回归，实际上是一种分类学习方法。

一种常用的对数几率函数如下：
> <b>y = 1/(1+e<sup>-z</sup>)
即Sigmod函数，形似S的函数。在神经网络中有重要作用。
<img src="{{ "/assets/ml/3_2.png" | prepend: site.baseurl }}" />


好处是：直接对分类可能性建模，无需事先假设数据分布，不仅预测类别，还能得到近似概率预测。
还有一些求导和求最优解的步骤。T_T



* 线性判别分析（LDA）

适用于分类。

定义：设法将训练样本投影到一条直线，相同样例的投影点尽可能近，不同样例的投影点尽可能远。有新样本来“预测”分类时，根据投影的点判断所属的分类。
可以将LDA推广到N个类的分类。由于使用投影的概念，可以减少原始样本的属性个数，实现“降维”。LDA也常被视为一种经典的监督降维技术。

* 多分类学习

* 类别不平衡










预告
决策树，行为树在AI中的使用

附
* [线性代数学习笔记](https://github.com/zlotus/notes-linear-algebra)