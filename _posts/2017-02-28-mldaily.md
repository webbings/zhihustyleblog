---
layout: post
title: ML笔记
author: Jason
date:   2017-02-28
categories: ComputerVision
permalink: /archivers/MLDaily
---



<p class="lead">学习周志华老师的《机器学习》笔记，感想</p>

机器学习是当前非常热门的学习方向，说来其实有缘，我的本科毕设就是和机器学习相关的，当时偶然听说了“推荐系统”的名词，然后网上一搜资料，去图书馆借了本《推荐系统实践》的书，研习之后，就找了导师直接确定方向开搞了。说心里话，这本书写的真好，不仅简单通俗的给我介绍了推荐系统的发展历史、背景、热门技术和未来展望，还提供了生动鲜明的例子，让读者容易理解笑话，最后论文还引用了很多此书的内容。后来和导师一起琢磨了半年，算是做了些东西。

附：[一种基于信任关系的物品推荐方法](https://www.google.com/patents/CN104915391A?cl=zh)

回到《机器学习》这本书，作者周志华当然是国内机器学习领域响当当的大牛，不仅科研厉害，教书也很厉害。我一直坚信，最厉害的掌握知识的方法不是钻在研究各种奇淫巧技，而是能够将自己的优秀的方法和见解教给别人，而且要讲的清楚，别人能听得懂。就像电影《Margin Call》里面大boss说的：tell me like i'm 5 years old。这应该是我学习工作的目标。“华哥”应该就是这类致力于把复杂的东西说简单的一类人。

《机器学习》和《推荐系统实践》类似，介绍了机器学习作为人工智能中发展最快的分支之一，经历的各种历程和当前的主流研究方向，作为一本教科书，里面充斥着各种公式，看起来挺费劲。纵观机器学习的历史，从最早的符号学习，到后来的“黑马”机器学习（国内代表人物是华为诺亚方舟实验室的主任李航博士），还有模拟人脑神经元的神经网络，到当下最时髦的深度学习（多层神经网络和多层隐式神经网络等），每一次的波峰，都是伴随着富有创新的算法或者模型的出现，例如支持向量机SVM，贝叶斯分类器，BP反向传播算法等，看一些历史，还发现技术和人一样，在历史的浪潮中起起伏伏，真有“三十年河东，三十年河西”的感觉。例如多层神经网络在90年代鲜有人问津，因为那时风头最旺的是SVM为代表的统计机器学习，但是Hinton教授一直没有放弃相关的研究，在2005年，发表论文宣告多隐层的神经网络具有优异的特征学习能力，学习得到的特征对数据有更本质的刻画，从而有利于可视化或分类；深度神经网络在训练上的难度，可以通过“逐层初始化” 来有效克服。之后深度学习又进入了波峰。

读序言，作者和一些学术界的大牛的观点认为：虽然当前“深度学习”在社会上关注很高，但是并不是就说它们代表了机器学习的新方向。其实道理也简单，深度学习的热潮最核心不是有多么牛逼的算法和理论创新，而是得益于计算机的计算能力提升了，使得同学们的算法和模型里的参数能够更好的优化，即参数调优做的好，适配当前数据集而已。作为一个抱着读科普书态度的我，我觉得有兴趣的就看看摸索摸索，了解了解它们的原理，找找实际的demo玩玩，有能力的话可以针对一个领域做些研究，比如图像处理或者数据挖掘等，或者在推荐系统中结合机器学习做些探索......

附：[用深度学习（DNN）构建推荐系统 - Deep Neural Networks for YouTube Recommendations论文精读](https://zhuanlan.zhihu.com/p/25343518)


抱着读科普书态度的我就不要想着通过这本入门级别的教科书就能成为机器学习的专家了，能细细学习就不错了:)。看了一些还是有些收获的，例如在模型评估与选择中，看到了熟悉的P和R，即精确度和召回率，这正是我之前验证推荐系统好坏所使用的判断方法，其中的F1度量，F1=2xPxR/(P+R)，当时还以为是别人论文里自己发明的呢，原来这些东西早就有了。。。

书里也提到了一些很复杂模型评估方法，例如ROC和AUC等，通过设置分类阈值ROC曲线，进而计算面积求得AUC，以此来判断性能好坏；除此之外，还讲到一些交叉验证的方法，感觉上去效果会更好，涉及到噪声期望、方差等就比较高级了。。。

书中还有许多牛逼的机器学习算法值得去细看。



参考文献
* [简论人工智能](http://blog.sina.com.cn/s/blog_7ad48fee0102w4bg.html)
* [从机器学习谈起](http://www.cnblogs.com/subconscious/p/4107357.html)
